{
    "componentChunkName": "component---src-templates-blog-article-tsx",
    "path": "/blog/azure-intro-kubernetes",
    "result": {"data":{"mdx":{"frontmatter":{"title":"How to use Kubernetes with Microsoft Azure and GitHub and how to debug if it does not workout","date":"July 31, 2022"},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"How to use Kubernetes with Microsoft Azure and GitHub and how to debug if it does not workout\",\n  \"date\": \"2022-07-31\",\n  \"categories\": [\"azure\", \"docker\", \"container\", \"kubernetes\", \"helm\"]\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar TocAzureContainerSeries = makeShortcode(\"TocAzureContainerSeries\");\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Kubernetes is a convenient way to deploy an infrastructure of services in a central place. Once you have your service Docker images built and deployed you can use Kubernetes to deploy multiple instances across the cloud. In this article, we will see how Microsoft Azure can use a Kubernetes configuration generated with Helm and deploy many images from Azure Registry Container.\"), mdx(\"h1\", null, \"Create an Azure Kubernetes Service (AKS)\"), mdx(\"p\", null, \"Few informations is required that was build when we created our Azure Docker Image Repository.\\nYou need to use the Azure resource group name after \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"--resource-group\"), \".  The name of the Kubernetes service is after \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"--name\"), \". The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"--attach-acr\"), \" is the name of the Azure Container Registry (acr). \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"az aks create --resource-group realtimepixel_resourcegroup --name realpixelask --location eastus --attach-acr realtimepixel --generate-ssh-keys\\n\")), mdx(\"p\", null, \"There is few things to know:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"The name cannot have underscore. Even if you enclose with double quote it does not work\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"The command takes a while to run. Expect at least 1 minute.\")), mdx(\"p\", null, \"Most online tutorial shows to use the Azure CLI (\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"az\"), \" command) but it is also possible to create the Kubernetes service on the Microsoft Azure Portal.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/809f0ebf184eb60e1ede80526ae676db/84f4d/azure_kbs_creation.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"63.66666666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAB9UlEQVQ4y3WTS2sUURCF59e6EYyIv8OVLlwZ/QGKgoKQlQvRoCAuRIQJmkBiZvp1+75f/cm9mZ5MHjYU9/ah6tQ5Vd2LB08OePj4HXcevefu0+/s7R+x93zJ/f0j7j1b8uLDX958XfHycMWrwxWvv6x5+637byz+/PzI8a9PnC0/M7mGHED2knZ9jrOalDIxTeQJUi4x1fdyXg0qvjj+cYAYOrRN5GlCSsfvk4aTszWrTqGtJ6WID4EYIzlFYko4H2oU3IeIs5acEotmdYoxqgIheLpecXp2jhADWmusNSilEEKgTEAoh7KhKk8pbc5MjImcM4u2HdBKYbRCK4kQCqUMtnTMmfKUewwBMSpGZXAhMXH7s5imiTlqsfNIqaoqOY6VtCgt2DAamsHQCU0vdFV4g3C+zB29D4yjRCm5LTDFtjaVpOkl2pg6Cuc9zjm883jva/4NhUYF2qZHa7XFnIkM/UDb9YQQyDnVBZUI87JyrvlV4S6hUoG+HzHm0lKKmb4X1XbJK6OwxuCdqwoL6RXLu4TORrQ2pHw5nxAi605hXKjD2V3YdmQbjhuE1jqEGDfW8gaf6ldQ5lQwKWW9X6+93bKNdL1gFGKrQrlEIxzGxUo+N5qbzbXXLF/s2rqIGC8UzInl12rbjn4Y6lzFTrNdskL4Dy+e6uP+X+0fAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"azure kbs creation\",\n    \"title\": \"azure kbs creation\",\n    \"src\": \"/static/809f0ebf184eb60e1ede80526ae676db/c1b63/azure_kbs_creation.png\",\n    \"srcSet\": [\"/static/809f0ebf184eb60e1ede80526ae676db/5a46d/azure_kbs_creation.png 300w\", \"/static/809f0ebf184eb60e1ede80526ae676db/0a47e/azure_kbs_creation.png 600w\", \"/static/809f0ebf184eb60e1ede80526ae676db/c1b63/azure_kbs_creation.png 1200w\", \"/static/809f0ebf184eb60e1ede80526ae676db/84f4d/azure_kbs_creation.png 1208w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"h1\", null, \"Configure Github Workflow\"), mdx(\"p\", null, \"You can edit the existing Github workflow defined previously that was building the image and pushing it into Azure Container Registry (ACR). However, I decided to create a new workflow allowing me to decide manually when to push the image into production.\"), mdx(\"p\", null, \"Here is the full Github workflow that I store into the repository in \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \".github/workflows/k8sdeploy.yml\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"on:\\n  workflow_dispatch:\\n\\n# Environment variables available to all jobs and steps in this workflow\\nenv:\\n  REGISTRY_NAME: realtimepixel\\n  CLUSTER_NAME: realpixelask\\n  CLUSTER_RESOURCE_GROUP: realtimepixel_resourcegroup\\n  NAMESPACE: realtimepixel-prod\\n\\njobs:\\n  build:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@main\\n\\n      # Set the target Azure Kubernetes Service (AKS) cluster.\\n      - uses: azure/aks-set-context@v1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n          cluster-name: ${{ env.CLUSTER_NAME }}\\n          resource-group: ${{ env.CLUSTER_RESOURCE_GROUP }}\\n\\n      # Create namespace if doesn't exist\\n      - run: |\\n          kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o json | kubectl apply -f -\\n\\n      - name: Helm tool installer\\n        uses: Azure/setup-helm@v1\\n\\n      - name: Azure Login\\n        uses: Azure/login@v1.1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n      - name: Get Latest Tag Redis\\n        id: latesttagredis\\n        run: |\\n          tag_redis=$(az acr repository show-tags --name ${{env.REGISTRY_NAME}} --repository realtimepixel_redis --top 1 --orderby time_desc -o tsv)\\n          echo \\\"::set-output name=tag_redis::$tag_redis\\\"\\n      \\n      - name: Tag Redis\\n        run: echo \\\"Tag Redis is ${{ steps.latesttagredis.outputs.tag_redis }}\\\"\\n\\n      - name: Get Latest Tag Backend\\n        id: latesttagbackend\\n        run: |\\n          tag_backend=$(az acr repository show-tags --name ${{env.REGISTRY_NAME}} --repository realtimepixel_backend --top 1 --orderby time_desc -o tsv)\\n          echo \\\"::set-output name=tag_backend::$tag_backend\\\"\\n\\n      - name: Tag Backend\\n        run: echo \\\"Tag Backend is ${{ steps.latesttagbackend.outputs.tag_backend }}\\\"\\n      \\n      - name: Get Latest Tag Frontend\\n        id: latesttagfrontend\\n        run: |\\n          tag_frontend=$(az acr repository show-tags --name ${{env.REGISTRY_NAME}} --repository realtimepixel_frontend --top 1 --orderby time_desc -o tsv)\\n          echo \\\"::set-output name=tag_frontend::$tag_frontend\\\"\\n\\n      - name: Tag Frontend\\n        run: echo \\\"Tag Frontend is ${{ steps.latesttagfrontend.outputs.tag_frontend }}\\\"\\n\\n      - name: Deploy\\n        run: >\\n          helm upgrade realtimepixel ./kubernetes/realtimepixel \\n          --install \\n          --namespace=${{ env.NAMESPACE }} \\n          --set namespace=${{env.NAMESPACE}}\\n          --set image.pullpolicy=IfNotPresent\\n          --set image.redis.repository=${{env.REGISTRY_NAME}}.azurecr.io/realtimepixel_redis\\n          --set image.redis.tag=${{ steps.latesttagredis.outputs.tag_redis }}\\n          --set image.backend.repository=${{env.REGISTRY_NAME}}.azurecr.io/realtimepixel_backend\\n          --set image.backend.tag=${{ steps.latesttagbackend.outputs.tag_backend }}\\n          --set image.frontend.repository=${{env.REGISTRY_NAME}}.azurecr.io/realtimepixel_frontend\\n          --set image.frontend.tag=${{ steps.latesttagfrontend.outputs.tag_frontend }}\\n\\n\")), mdx(\"p\", null, \"Here is a description of what is going on:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The first section called \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"env\"), \" are variable that can be used across the whole workflow. It is a simple way to configure data in a central place for the script. It defined the registry (Azure Container Registry) name, the Kubernetes cluster name created in this blog post, the Azure resource group (previous article) and the Kubernetes namespace.\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The second section connect to AKS: Azure Kubernetes Service\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"We create the namespace\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"We then connect with Helm and Azure Login. From now, we are ready to perform some commands\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"First, we get the latest image tag for each image\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Finally, we use \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"helm\"), \" to install or update the Kubernetes configuration. What is important is to override a lot of values from \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"kubernetes\\\\realtimepixel\\\\values.yaml\"), \" (Helm values file)\"))), mdx(\"h1\", null, \"Verification\"), mdx(\"p\", null, \"At this point, the Helm command pushes the instruction to Azure Kubernetes Service. Going in the portal you can see under \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Workloads\"), \" the deployment.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1075px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/8d98a6dcd3a92cb1305235ebb347abe0/38a65/azure_k8s_workloads.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"55.666666666666664%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAB4UlEQVQoz4WSy27UQBRE/dtIiAWCDXt+AAkIEknIij8IUiA7FkQISIAh4xm77X6/bB/UPcmgICRKuvK176Oryt3ce3HJw9dXPD94ypOjT7w87Xh1uubwbMPR2Zbj9x0n5z0n56I+33zo7kSpPzq84v7BNx4fXdGMg2C1WrFZfabbtijjmOcF2Xs2bUu7vkZJiRwHjNH8D40PASkVNmSMMeScWJYZJR1d16O1JsZIypmUSqSbyPgQd+EDPiScczTW2ToUMnUo3wxK5er3ZVmY57merrRBaVt7vPc1nPOEGPd5I7Yjm3aD+nKMlRuMsWilENJX5oXdLcQw0m57hBCVzTRNGK3pux6lVH1vUpyw1uPFBTk6cp5IMeCsJ4RQ5ZWlhWmKEWtNZXMr31pbGZbewrz5l7ELII2l7zuEGOpQXZgi1oeaF5TFxfcC5zUhBppS3MW8b5zsNbLfMoySGALzPDFNuebCJLYqEfNS2Rb5Bb3+hQ/2D8PbpfW0729ZXZzzc7Vm6Du0ksS4k2hcRPupXq0is7Av0H4gpnBX8o7gQp5B6rw3fpkz7usB63cP6NcXtAZMKHVL27bVw2QHYnB/M9xJLhLHQe+vTfVrvKT7+AwlfiA9pGmuFkg51h+Ug6ke/wZAU0mgBYEzZAAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"azure k8s workloads\",\n    \"title\": \"azure k8s workloads\",\n    \"src\": \"/static/8d98a6dcd3a92cb1305235ebb347abe0/38a65/azure_k8s_workloads.png\",\n    \"srcSet\": [\"/static/8d98a6dcd3a92cb1305235ebb347abe0/5a46d/azure_k8s_workloads.png 300w\", \"/static/8d98a6dcd3a92cb1305235ebb347abe0/0a47e/azure_k8s_workloads.png 600w\", \"/static/8d98a6dcd3a92cb1305235ebb347abe0/38a65/azure_k8s_workloads.png 1075w\"],\n    \"sizes\": \"(max-width: 1075px) 100vw, 1075px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"p\", null, \"Everything should be running as expected! The screenshot shows three orange symbols next to my three deployments because there is an issue with the Kubernetes configuration which is outside the goal of this post. However, it is still interesting to know that you can drill and see the error reason being: \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"ErrImageNeverPull\"), \".\"), mdx(\"h2\", null, \"Debug ErrImageNeverPull\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"kubectl get pods -n realtimepixel-prod\\nkubectl describe pod redis-deployment-6495cd48cc-fhzjg -n realtimepixel-prod\\n\")), mdx(\"p\", null, \"The last command gives more information saying the policy is to \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Never\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Events:\\nType     Reason             Age                    From               Message\\n----     ------             ----                   ----               -------\\nNormal   Scheduled          59m                    default-scheduler  Successfully assigned realtimepixel-prod/redis-deployment-6495cd48cc-fhzjg to aks-agentpool-28884595-vmss000002\\nWarning  Failed             57m (x12 over 59m)     kubelet            Error: ErrImageNeverPull\\nWarning  ErrImageNeverPull  4m36s (x260 over 59m)  kubelet            Container image \\\"realtimepixel.azurecr.io/realtimepixel_redis:67dfbf27b868bd0b9e7c77aefafb596f2adb3ca0\\\" is not present with pull policy of Never\\n\")), mdx(\"p\", null, \"Trying to see what is really sent to Kubernetes using the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"template\"), \" command:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"helm template realtimepixel ./kubernetes/realtimepixel --set namespace=realtimepixel-prod --set image.pullPolicy=Always --set image.redis.repository=realtimepixel.azurecr.io/realtimepixel_redis --set image.redis.tag=123123 --set image.backend.repository=realtimepixel.azurecr.io/realtimepixel_backend --set image.backend.tag=123123 --set image.frontend.repository=realtimepixel.azurecr.io/realtimepixel_frontend --set image.frontend.tag=123123 > temp.yaml\\n\")), mdx(\"p\", null, \"I found a case sensitive issue with the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"image.pullPolicy\"), \".\"), mdx(\"h2\", null, \"Debug CrashLoopBackOff\"), mdx(\"p\", null, \"There is a command to get the health of all your pods. In my case, some of them were crashing:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"kubectl get pods -n realtimepixel-prod\\n\")), mdx(\"p\", null, \"Resulted with: \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"NAME                                   READY   STATUS             RESTARTS         AGE\\nbackend-deployment-69c99548d9-g2w5d    0/1     CrashLoopBackOff   10 (3m28s ago)   30m\\nbackend-deployment-69c99548d9-wrhzp    0/1     CrashLoopBackOff   10 (3m42s ago)   30m\\nbackend-deployment-7dfbc4f7f8-m99kl    0/1     CrashLoopBackOff   10 (3m51s ago)   109m\\nbackend-deployment-7dfbc4f7f8-vbp24    0/1     CrashLoopBackOff   10 (4m4s ago)    109m\\nfrontend-deployment-6f88fdb587-2p2lk   1/1     Running            0                30m\\nredis-deployment-5d48cc44bd-8w869      1/1     Running            0                30m\\n\")), mdx(\"p\", null, \"kubectl describe pod backend-deployment-69c99548d9-g2w5d -n realtimepixel-prod\"), mdx(\"p\", null, \"kubectl logs backend-deployment-69c99548d9-g2w5d -n realtimepixel-prod\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"> start:production\\n> node -r ts-node/register/transpile-only -r tsconfig-paths/register build/backend/src/index.js\\n\\nError: Cannot find module '/node/build/backend/src/index.js'\\n    at Function.Module._resolveFilename (node:internal/modules/cjs/loader:933:15)\\n    at Function.Module._resolveFilename.sharedData.moduleResolveFilenameHook.installedValue (/node/node_modules/@cspotcode/source-map-support/source-map-support.js:679:30)\\n    at Function.Module._resolveFilename (/node/node_modules/tsconfig-paths/src/register.ts:90:36)\\n    at Function.Module._load (node:internal/modules/cjs/loader:778:27)\\n    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:77:12)\\n    at node:internal/main/run_main_module:17:47 {\\n  code: 'MODULE_NOT_FOUND',\\n  requireStack: []\\n}\\n\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"kubectl exec -it backend-deployment-69c99548d9-g2w5d -n realtimepixel-prod -- sh\\n\")), mdx(\"p\", null, \"But was producing:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \" error: unable to upgrade connection: container not found...\\n\")), mdx(\"p\", null, \"You can test it by creating a Pod for the problematic image that will not restart using the following commands:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"kubectl run debug-demo -n realtimepixel-prod --image=realtimepixel.azurecr.io/realtimepixel_backend:67dfbf27b868bd0b9e7c77aefafb596f2adb3ca0 --restart=Never\\nkubectl get pods -n realtimepixel-prod\\nkubectl exec -it debug-demo -n realtimepixel-prod -- sh\\n\")), mdx(\"p\", null, \"However, the problem will remain that when the Kube Control runs the image that it will crash. However, the log above is providing good information. In my case I realized two things:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"When testing locally, I wasn't testing properly. The build was passing because I had a \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"node_modules\"), \" with a dependency that was installed when performing \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"npm run install\"), \" which was adding all the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"devDependencies\"), \". On Github, performing the same command, with the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"NODE_ENV\"), \" to \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"production\"), \" was causing \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"npm install\"), \" to install only the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"dependencies\"), \" without the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"devDependencies\"), \".\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"I added the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"--target\"), \" in the build to ensure that only the production multi-stage is executed\")), mdx(\"h1\", null, \"Still Not Working!\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"docker build -f ./services/backend/Dockerfile --target production --build-arg NODE_ENV=production .\\ndocker images\\ndocker run 40d4941a9092\\ndocker ps\\n\")), mdx(\"p\", null, \"Take the image id from the ps command:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"docker run -it 40d4941a9092 bash\\n\")), mdx(\"p\", null, \"I could see the error. Now time to make the container not crash but to get into:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"docker run 40d4941a9092 /bin/sh -c \\\"while true; do sleep 2; df -h; done\\\"\\n\")), mdx(\"p\", null, \"In another console:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"docker run -it 40d4941a9092 bash\\n\")), mdx(\"p\", null, \"At that point, I saw that the build was messing around the folders of the ouput of TypeScript. I modified the path and was good to go.\"), mdx(\"h1\", null, \"Conclusion\"), mdx(\"p\", null, \"At this point, the image was created with a build that was successful. \"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/0fdef9e386c397248d3019796c446231/e4611/azure_kubernetes_deployement.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"32%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA20lEQVQY04VQXXMDIQj0///MdvLQSxM/UFA83Q7mLmn6UmYcFZZdWPe9ET4+L9i2K1JKICLMiX9jzvl2zpyjlEGU0Lti33eIVNRa0ZqCWVBYwGI3o4pgjPFG8JfchVggwk9lHwk+JDAzVBUisqa2vwlqa8g5r5oRmMDvcJl4EdpUBrDGEDxKKYvA8maF3b33VY8xPgSbrpyRG9beLiVCCGGpGqEVuXZUHYf6y9DW2uHxYz0qYWEsfxK769cN3t/WKl0VrVZE7vBFMcZ8rTXnavTeH+sOJL4/MaeHPyPN1f3eJjgdAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"azure kubernetes deployement\",\n    \"title\": \"azure kubernetes deployement\",\n    \"src\": \"/static/0fdef9e386c397248d3019796c446231/c1b63/azure_kubernetes_deployement.png\",\n    \"srcSet\": [\"/static/0fdef9e386c397248d3019796c446231/5a46d/azure_kubernetes_deployement.png 300w\", \"/static/0fdef9e386c397248d3019796c446231/0a47e/azure_kubernetes_deployement.png 600w\", \"/static/0fdef9e386c397248d3019796c446231/c1b63/azure_kubernetes_deployement.png 1200w\", \"/static/0fdef9e386c397248d3019796c446231/e4611/azure_kubernetes_deployement.png 1298w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"p\", null, \"I would say that the experience was enriching. However, one question kept getting in the back of my mind: why isn't the Azure Portal guiding me with more than a single keyword for the failure. As we saw, by messing around with commands, we found the root cause, but some support would have been great. Nonetheless, if something happens to you, now you should be equipped to diagnose a little bit better.\"), mdx(TocAzureContainerSeries, {\n    mdxType: \"TocAzureContainerSeries\"\n  }));\n}\n;\nMDXContent.isMDXComponent = true;"}},"pageContext":{"id":"6984d70e-a653-5216-acd8-7a67976eb8b5","totalPages":73}},
    "staticQueryHashes": ["3159585216"]}